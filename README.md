# Fine-Tuning GPT-2 for News Generation

This project fine-tunes the GPT-2 language model on a news dataset for conditional text generation.

## ğŸ“ Project Structure
---
â”œâ”€â”€ news_dataset.jsonl # Training data (one JSON object per line, with "text" key)
â”œâ”€â”€ hoasens_tokenize.py # Tokenizes dataset and saves to disk
â”œâ”€â”€ train_model.py # Fine-tunes GPT-2 using Hugging Face Trainer
â”œâ”€â”€ train_llm.sh # Script to launch training
â”œâ”€â”€ evaluate.py # Inference script to generate text
â”œâ”€â”€ my_finetuned_gpt2/ # Saved model output after training
â”œâ”€â”€ my_finetuned_gpt2_datasets/ # Tokenized dataset
â”œâ”€â”€ my_finetuned_gpt2_tokenizer/ # Tokenizer config
â””â”€â”€ README.md
---

## ğŸ”§ Setup

Create and activate a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

Install dependencies:
```
pip install -r requirements.txt
```

ğŸš€ Train the Model
```
./train_llm.sh
```

ğŸ“Š Evaluate the Model
```
python3 evaluate.py
```